output_root: "/kaggle/working/output/"
data_dir: "/kaggle/working/" # Updated path for Potsdam dataset
experiment_name: "exp1"
log_dir: "loggs" # Changed from cityscapes to potsdam
checkpoint_dir: "checkpts"
azureml_logging: True

# Loader params
num_workers: 2
max_steps: 21 # 3000 8000   no . of training steps
batch_size: 32

dataset_name: "potsdam" # Updated to potsdam
crop_type: "five"
crop_ratio: .5
res: 224
loader_crop_type: "center"

# Model Params
extra_clusters: 0
model_type: "vit_base" # "vit_base" "vit_small" "mocov2"
arch: "dino" # "dinov2"  "dino"
dino_feat_type: "feat"
projection_type: "nonlinear"
dino_patch_size: 8
n_feats: 768 # 2048 # 384 # 768  size of the output space
dim: 64
run_crf: False

lr1: 1e-4
lr2: 5e-4
dropout: True

temperature: 0.1
alpha: 0.998

pointwise: True
seed: 0

# potsdam dino base
neg_inter_shift: 0.5
pos_intra_shift: 0.1

# potsdam dinov2 base
#neg_inter_shift: 0.5
#pos_intra_shift: 0.32

# Logging params
n_images: 6 # number of images to use for logging
scalar_log_freq: 10 # how frequently scalar values (such as loss or accuracy metrics) are logged
checkpoint_freq: 400 #how often (in terms of steps or iterations) the model’s state (weights and optimizer state) should be saved as a checkpoint
val_freq: 400 #how frequently the model’s performance should be evaluated on a validation set
hist_freq: 100 # training data will be logged every 100 steps

hydra:
  run:
    dir: "/kaggle/working/output"
  output_subdir: "${hydra.run.dir}/${now:%Y-%m-%d_%H-%M-%S}"
# hydra:
#   run:
#     dir: "."
#   output_subdir: ~
